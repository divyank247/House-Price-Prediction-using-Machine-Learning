# -*- coding: utf-8 -*-
"""House Price Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1m5NpvE926uxnYlJqyhESHTys0A7JRwTU
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import sklearn.datasets
from sklearn.model_selection import train_test_split, GridSearchCV
from xgboost import XGBRegressor
from sklearn import metrics
from sklearn.preprocessing import StandardScaler, PolynomialFeatures

house_price_dataset = sklearn.datasets.fetch_california_housing()

#Loading the dataset to Pandas Dataframe
house_price_dataframe = pd.DataFrame(house_price_dataset.data, columns = house_price_dataset.feature_names)

house_price_dataframe.head()

#adding the target(price) column to the DataFrame
house_price_dataframe['price'] = house_price_dataset.target

house_price_dataframe.head()

#checking the number of rows and columns in the data frame
house_price_dataframe.shape

#checking for missing values
house_price_dataframe.isnull().sum()

#statistical measures of the dataset
house_price_dataframe.describe()

correlation = house_price_dataframe.corr()

# Constructing a heatmap to understand the correlation
plt.figure(figsize=(10, 10))
sns.heatmap(correlation, cbar=True, square=True, fmt='.1f', annot=True, annot_kws={'size': 8}, cmap='Blues')
plt.title('Correlation Heatmap')
plt.show()

# Feature Engineering: Adding polynomial features for selected columns
poly = PolynomialFeatures(degree=2, include_bias=False)
poly_features = poly.fit_transform(house_price_dataframe[['MedInc', 'AveRooms', 'HouseAge']])
poly_features_df = pd.DataFrame(poly_features, columns=poly.get_feature_names_out(['MedInc', 'AveRooms', 'HouseAge']))

# Concatenating the polynomial features with the original DataFrame
house_price_dataframe = pd.concat([house_price_dataframe.drop(['MedInc', 'AveRooms', 'HouseAge'], axis=1), poly_features_df], axis=1)

# Removing any duplicate columns that might have been added
house_price_dataframe = house_price_dataframe.loc[:, ~house_price_dataframe.columns.duplicated()]

"""Splitting the data and Target"""

# Splitting the data into Features (X) and Target (Y)
X = house_price_dataframe.drop(['price'], axis=1)
Y = house_price_dataframe['price']

"""Splitting the data into Training data and Test data"""

# Splitting the data into Training data and Test data
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=2)

# Standardizing the features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# Model Training with XGBoost Regressor
model = XGBRegressor()

# Training the model with X_train
model.fit(X_train, Y_train)

# Hyperparameter Tuning using GridSearchCV
param_grid = {
    'n_estimators': [50, 100, 150],
    'learning_rate': [0.1, 0.05, 0.01],
    'max_depth': [3, 5, 7],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring='r2', cv=5, verbose=1)
grid_search.fit(X_train, Y_train)

# Best parameters from grid search
best_model = grid_search.best_estimator_

# Prediction on test data
test_data_prediction = best_model.predict(X_test)

# R squared error for test data
score_1 = metrics.r2_score(Y_test, test_data_prediction)
score_2 = metrics.mean_absolute_error(Y_test, test_data_prediction)

print(f"R squared error (Test): {score_1}")
print(f"Mean Absolute Error (Test): {score_2}")

# Predicting with new input data
# First, generating polynomial features for the new input
new_input = np.array([[8.3014,	21.0,	6.238137]])
new_poly_features = poly.transform(new_input)

# Concatenating the polynomial features with the remaining features
full_new_input = np.hstack([new_poly_features, np.array([[0.971880,	2401.0,	2.109842,	37.86,	-122.22]])])

# Scaling the full new input
full_new_input_scaled = scaler.transform(full_new_input)

# Making the prediction
prediction = best_model.predict(full_new_input_scaled)
print(f"Prediction for new input: {prediction}")

